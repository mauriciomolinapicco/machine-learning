{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5a4c2e-1082-4e49-91cd-c1ec2abb237e",
   "metadata": {},
   "source": [
    "# INTEGRANTES:\n",
    "#####            -GONZALO DANIEL GRECCO\n",
    "#####            -MAURICIO NICOLÁS MOLINA PICCO\n",
    "#####            -MARÍA INÉS BERDIÑAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b98d7",
   "metadata": {},
   "source": [
    "# Módulo 6: Redes Neuronales - Parte II\n",
    "\n",
    "## Redes Neuronales para clasificación multiclase\n",
    "\n",
    "En esta actividad vamos a aprender a desarrollar una red neuronal profunda para reconocimiento de imágenes.\n",
    "\n",
    "Además, estudiaremos algunos detalles de implementación que nos ayudarán a organizar mejor nuestro código y hacerlo más eficiente.\n",
    "\n",
    "Por otro lado, aprenderemos a utilizar **Callbacks** para detener el proceso de entrenamiento anticipadamente si se alcanza un objetivo deseado.\n",
    "\n",
    "Para ello, vamos a trabajar con un dataset provisto por Keras: **Fashion MNIST**. Este dataset consta de **70.000 imágenes en escala de grises de 28x28 píxeles**, pertenecientes a una de **10 clases de prendas de vestir** diferentes.\n",
    "\n",
    "El dataset está dividido en **60.000 imágenes para entrenamiento y 10.000 para testeo**.\n",
    "\n",
    "Cada imagen representa una prenda de vestir de una de las siguientes categorías:\n",
    "\n",
    "                                0. T-shirt/top\n",
    "                                1. Trouser\n",
    "                                2. Pullover\n",
    "                                3. Dress\n",
    "                                4. Coat\n",
    "                                5. Sandal\n",
    "                                6. Shirt\n",
    "                                7. Sneaker\n",
    "                                8. Bag\n",
    "                                9. Ankle boot \n",
    "\n",
    "Cada píxel tiene un valor asociado entre 0 y 255 que indica su nivel de luminosidad u oscuridad (valores más altos indican mayor oscuridad).\n",
    "\n",
    "El dataset tiene **785 columnas**, donde la primera corresponde al **label** (la categoría de la prenda) y las 784 restantes representan los features, es decir, los valores de cada uno de los píxeles de la imagen.\n",
    "\n",
    "La idea es utilizar este dataset para **reconocer prendas de vestir mediante una red neuronal profunda**, compuesta por varias capas (**layers**) y múltiples neuronas. Para ello, será necesario definir un **modelo de clasificación multiclase**.\n",
    "\n",
    "En primer lugar, vamos a **cargar el dataset en memoria**, dividiéndolo en conjunto de entrenamiento y conjunto de testeo, e imprimiremos sus dimensiones para confirmar que se haya cargado correctamente.\n",
    "\n",
    "\n",
    "### <font color='red'>**Actividad 1:**</font>\n",
    "**a)** Escribir un programa en Python para obtener, desde el framework Keras, **el dataset Fashion MNIST dividido en conjunto de entrenamiento (X_train, y_train) y conjunto de testeo (X_test, y_test)**.\n",
    "\n",
    "**AYUDA:** el dataset fashion_mnist se encuentra dentro de la libreria keras.datasets. Keras provee una forma de obtener el dataset dividido en entrenamiento y testeo con el método [.load_data() ](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data#expandable-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4083004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Carga del dataset (ya dividido en train y test)\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "#Carga el dataset. load_data() devuelve dos tuplas: la primera (X_train, y_train) para entrenamiento y la segunda (X_test, y_test) para testeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8ec82",
   "metadata": {},
   "source": [
    "**b)** Imprimir las **dimensiones** de todos los datasets (X_train, y_train, X_test, y test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7516835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dimensiones (shapes) ---\n",
      "X_train.shape: (60000, 28, 28)\n",
      "y_train.shape: (60000,)\n",
      "X_test.shape:  (10000, 28, 28)\n",
      "y_test.shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Dimensiones (shapes) ---\")\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape: \", X_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7b640-217c-4102-9e1d-bbf53a3a8054",
   "metadata": {},
   "source": [
    "### El resultado de shape de X es (60000,28,28) significa que son:\n",
    "- 60000 muestras (imagenes)\n",
    "- 28x28 tamanio de imagenes\n",
    "- la otra dimension no se muestra porque es 1 (1 dimension para color) pero si fuera rgb tendria otra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d6ad7",
   "metadata": {},
   "source": [
    "### <font color='red'>**Actividad 2:**</font>\n",
    "**a)** Imprimir dos imágenes cualquiera utilizando la librería **matplotlib**.\n",
    "\n",
    "**AYUDA:** Recordemos que para imprimir una imagen con matplotlib se puede utilizar el metodo [.imshow(image)](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html#matplotlib-pyplot-imshow), siendo image el vector que contiene los 784 pixeles de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a5d318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt #matplotlib es la biblioteca de graficos \n",
    "\n",
    "# Mostrar la primera imagen de entrenamiento\n",
    "plt.subplot(1, 2, 1)                # 1 fila, 2 columnas, primer gráfico\n",
    "plt.imshow(X_train[0], cmap=\"gray\") # X_train[0] es una matriz 28x28\n",
    "plt.title(f\"Etiqueta: {y_train[0]}\")\n",
    "plt.axis(\"off\")                     \n",
    "#plt.axis(\"off\") quita los ejes para que se vea más limpio\n",
    "\n",
    "# Mostrar la segunda imagen de entrenamiento\n",
    "plt.subplot(1, 2, 2)                # 1 fila, 2 columnas, segundo gráfico\n",
    "plt.imshow(X_train[1], cmap=\"gray\") # X_train[1] es otra imagen\n",
    "plt.title(f\"Etiqueta: {y_train[1]}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f20b58",
   "metadata": {},
   "source": [
    "### <font color='red'>**Actividad 3:**</font>\n",
    "**a)** **Normalizar los valores de los features** en los datasets de entrenamiento y testeo (X_train y X_test) dividiendo los valores de los pixeles por 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c828b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango X_train: min = 0.0 , max = 1.0\n",
      "Rango X_test:  min = 0.0 , max = 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Los píxeles en Fashion MNIST van de 0 a 255 (uint8).\n",
    "Al dividir entre 255, los llevamos al rango [0, 1], lo que hace que el entrenamiento sea más estable y eficiente.\n",
    "\"\"\"\n",
    "#astype(\"float32\") convierte los pixeles enteros a números decimales\n",
    "X_train = X_train.astype(\"float32\") / 255.0 \n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Verificamos resultados\n",
    "print(\"Rango X_train: min =\", X_train.min(), \", max =\", X_train.max())\n",
    "print(\"Rango X_test:  min =\", X_test.min(),  \", max =\", X_test.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6993e0b",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "Los Callbacks de Keras son objetos que ejecutan acciones en distintas etapas del proceso de entrenamiento (por ejemplo, al comienzo o al final de cada epoch). Generalmente, se utilizan para escribir registros (logs) de monitoreo, guardar el modelo en disco periódicamente, detener el entrenamiento anticipadamente, entre otras funciones.\n",
    "\n",
    "El siguiente código muestra cómo crear un Callback personalizado en TensorFlow/Keras que detiene el entrenamiento cuando se alcanza un valor máximo de precisión definido previamente:\n",
    "\n",
    "```\n",
    "class callbacks(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get(\"accuracy\") > MAX_ACC:\n",
    "            self.model.stop_training = True\n",
    "```\n",
    "\n",
    "Este tipo de callback se pasa como parámetro al método .fit() mediante el argumento callbacks.\n",
    "\n",
    "### <font color='red'>**Actividad 4:**</font>\n",
    "**a)** Crear una clase **Callback** que detenga el proceso de entrenamiento al finalizar un epoch si se alcanza un valor de accuracy igual o superior a 0.95, e imprima el siguiente mensaje de aviso:\n",
    "\n",
    "\t“Se alcanzó el máximo de precisión y, por lo tanto, se canceló el proceso de entrenamiento.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9cac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definición de Callback personalizado\n",
    "class EarlyStopAtAcc(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        acc = logs.get(\"accuracy\")\n",
    "        if acc is not None and acc >= 0.95:\n",
    "            print(\"\\nSe alcanzó el máximo de precisión y, por lo tanto, se canceló el proceso de entrenamiento.\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5cf506",
   "metadata": {},
   "source": [
    "### Ordenando el código\n",
    "Ahora que tenemos los datasets de entrenamiento y testeo normalizados, y hemos observado algunos ejemplos, vamos a crear el modelo, compilarlo y entrenarlo.\n",
    "\n",
    "Para ello, definiremos un método que reúna todo el proceso completo:\n",
    "1. Obtener el dataset de entrenamiento y de testeo (resuelto en Actividad 1.a.)\n",
    "2. Normalizar los features de entrenamiento y testeo (resuelto en Actividad 3.)\n",
    "3. Crear el modelo en Keras\n",
    "4. Compilar el modelo (.compile(...))\n",
    "5. Ajustar o entrenar (.fit(...)) el modelo pasandole el callback creado y almacenando el historial del proceso de entrenamiento.\n",
    "6. Retornar el historial del proceso de entrenamiento y el modelo creado.\n",
    "\n",
    "#### 🔧 Creación del modelo\n",
    "\n",
    "Vamos a construir una red neuronal con cuatro capas ocultas:\n",
    "- La primera con **512 neuronas**,\n",
    "- La segunda con **256 neuronas**,\n",
    "- La tercera con **128 neuronas**,\n",
    "- La cuarta con **64 neuronas**.\n",
    "\n",
    "Como se trata de un problema de **clasificación multiclase**, al final del modelo utilizaremos la función de activación **softmax**. Sin embargo, implementaremos la **versión mejorada** vista en el apunte: la última capa tendrá activación **lineal**, y la función **softmax** se aplicará luego, por fuera del modelo.\n",
    "\n",
    "Además, como estamos trabajando con imágenes, la primera capa de nuestra red debe ser una capa Flatten(), que transforma cada imagen 28x28 en un vector de 784 elementos.\n",
    "\n",
    "#### ⚙️ Compilación del modelo\n",
    "\n",
    "Al compilar el modelo:\n",
    "- Usaremos el optimizador **Adam** (optimizer='adam')\n",
    "- Como métrica de evaluación, emplearemos **accuracy** (metrics=['accuracy'])\n",
    "- Usaremos *SparseCategoricalCrossentropy(from_logits=True)* como función de pérdida ya que estamos utilizando logits como salida (activación lineal en la última capa)\n",
    "\n",
    "#### 🏋️ Entrenamiento del modelo\n",
    "\n",
    "Al entrenar el modelo con .fit(), le pasaremos:\n",
    "- Le pasaremos el **callback personalizado** creado anteriormente (callbacks=[myCallback])\n",
    "- Definiremos la cantidad de **epochs en 100** (epochs=100)\n",
    "\n",
    "#### 📈 Historial del entrenamiento\n",
    "\n",
    "El historial del proceso es devuelto por el método .fit() como un objeto History. Este objeto guarda, en su atributo .history, los registros de los valores de error y las métricas obtenidas en cada epoch.\n",
    "\n",
    "\n",
    "### <font color='red'>**Actividad 5:**</font>\n",
    "**a)** Crear un método llamado train_fashion_mnist() que englobe los seis pasos mencionados anteriormente. Notar que algunos de estos pasos ya fueron resueltos en ejercicios anteriores, por lo tanto, simplemente debemos **copiar las líneas de código correspondientes dentro de este nuevo método** para integrarlos al flujo completo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fde0f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.nn import softmax\n",
    "\n",
    "# Callback personalizado que detiene el entrenamiento al alcanzar 95% de accuracy\n",
    "class EarlyStopAtAcc(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        acc = logs.get(\"accuracy\")\n",
    "        if acc is not None and acc >= 0.95:\n",
    "            print(\"\\nSe alcanzó el máximo de precisión y, por lo tanto, se canceló el proceso de entrenamiento.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def train_fashion_mnist():\n",
    "\n",
    "    # Creamos un objeto de la clase callbacks creada anteriormente\n",
    "    myCallback = EarlyStopAtAcc()\n",
    "    \n",
    "    # 1. Obtener el dataset de entrenamiento y de testeo (resuelto en Ejercicio 1) a.)\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    # 2. Normalizar los features de entrenamiento y testeo (resuelto en Ejercicio 3.)\n",
    "    X_train = X_train.astype(\"float32\") / 255.0\n",
    "    X_test  = X_test.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # 3. Crear el modelo en Keras\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),   # Aplana la imagen 28x28 en vector 784\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"linear\")   # salida lineal\n",
    "    ])\n",
    "    \n",
    "    # 4. Compilar el modelo (.compile(...))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=SparseCategoricalCrossentropy(from_logits=True), \n",
    "        metrics=[\"accuracy\"]\n",
    "    ) #Cuando from_logits=True, Keras aplica internamente un softmax a los logits antes de calcular la entropía cruzada. De esta forma, no necesitamos poner activation=\"softmax\" dentro del modelo.\n",
    "    \n",
    "    # 5. Ajustar o entrenar (.fit(...)) el modelo pasandole el callback creado y almacenando el historial del proceso de entrenamiento.\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        callbacks=[myCallback],\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # 6.Retornar el historial del proceso de entrenamiento.\n",
    "    return history.history, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff64da4",
   "metadata": {},
   "source": [
    "Ahora que tenemos un método capaz de realizar todo el proceso completo, **lo único que resta es llamar a dicho método** para obtener el objeto history y el model.\n",
    "\n",
    "El entrenamiento debería finalizar aproximadamente en el **epoch 35**, si se alcanza el nivel de precisión definido en el callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77794dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 - 9s - 5ms/step - accuracy: 0.8190 - loss: 0.4975 - val_accuracy: 0.8526 - val_loss: 0.4035\n",
      "Epoch 2/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.8657 - loss: 0.3685 - val_accuracy: 0.8642 - val_loss: 0.3815\n",
      "Epoch 3/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.8776 - loss: 0.3336 - val_accuracy: 0.8525 - val_loss: 0.4016\n",
      "Epoch 4/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.8851 - loss: 0.3116 - val_accuracy: 0.8669 - val_loss: 0.3737\n",
      "Epoch 5/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.8942 - loss: 0.2902 - val_accuracy: 0.8766 - val_loss: 0.3531\n",
      "Epoch 6/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.8980 - loss: 0.2749 - val_accuracy: 0.8800 - val_loss: 0.3443\n",
      "Epoch 7/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9022 - loss: 0.2636 - val_accuracy: 0.8711 - val_loss: 0.3833\n",
      "Epoch 8/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9056 - loss: 0.2515 - val_accuracy: 0.8856 - val_loss: 0.3266\n",
      "Epoch 9/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9093 - loss: 0.2433 - val_accuracy: 0.8848 - val_loss: 0.3513\n",
      "Epoch 10/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9109 - loss: 0.2346 - val_accuracy: 0.8751 - val_loss: 0.3558\n",
      "Epoch 11/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9155 - loss: 0.2247 - val_accuracy: 0.8832 - val_loss: 0.3413\n",
      "Epoch 12/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9167 - loss: 0.2175 - val_accuracy: 0.8724 - val_loss: 0.3986\n",
      "Epoch 13/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9192 - loss: 0.2118 - val_accuracy: 0.8874 - val_loss: 0.3387\n",
      "Epoch 14/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9236 - loss: 0.2043 - val_accuracy: 0.8877 - val_loss: 0.3539\n",
      "Epoch 15/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9244 - loss: 0.1967 - val_accuracy: 0.8907 - val_loss: 0.3565\n",
      "Epoch 16/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9266 - loss: 0.1924 - val_accuracy: 0.8810 - val_loss: 0.3792\n",
      "Epoch 17/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9281 - loss: 0.1879 - val_accuracy: 0.8901 - val_loss: 0.3454\n",
      "Epoch 18/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9315 - loss: 0.1810 - val_accuracy: 0.8885 - val_loss: 0.3876\n",
      "Epoch 19/100\n",
      "1875/1875 - 9s - 5ms/step - accuracy: 0.9339 - loss: 0.1755 - val_accuracy: 0.8874 - val_loss: 0.3862\n",
      "Epoch 20/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9346 - loss: 0.1712 - val_accuracy: 0.8901 - val_loss: 0.3884\n",
      "Epoch 21/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9363 - loss: 0.1649 - val_accuracy: 0.8938 - val_loss: 0.3642\n",
      "Epoch 22/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9368 - loss: 0.1637 - val_accuracy: 0.8859 - val_loss: 0.4069\n",
      "Epoch 23/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9383 - loss: 0.1608 - val_accuracy: 0.8935 - val_loss: 0.3805\n",
      "Epoch 24/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9417 - loss: 0.1517 - val_accuracy: 0.8816 - val_loss: 0.4482\n",
      "Epoch 25/100\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9405 - loss: 0.1554 - val_accuracy: 0.8929 - val_loss: 0.3931\n",
      "Epoch 26/100\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9443 - loss: 0.1449 - val_accuracy: 0.8958 - val_loss: 0.4464\n",
      "Epoch 27/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9450 - loss: 0.1426 - val_accuracy: 0.8915 - val_loss: 0.4543\n",
      "Epoch 28/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9463 - loss: 0.1443 - val_accuracy: 0.8910 - val_loss: 0.4065\n",
      "Epoch 29/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9488 - loss: 0.1362 - val_accuracy: 0.8912 - val_loss: 0.4528\n",
      "Epoch 30/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9483 - loss: 0.1333 - val_accuracy: 0.8933 - val_loss: 0.4536\n",
      "Epoch 31/100\n",
      "1875/1875 - 8s - 4ms/step - accuracy: 0.9500 - loss: 0.1325 - val_accuracy: 0.8956 - val_loss: 0.4483\n",
      "Epoch 32/100\n",
      "\n",
      "Se alcanzó el máximo de precisión y, por lo tanto, se canceló el proceso de entrenamiento.\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9501 - loss: 0.1324 - val_accuracy: 0.8878 - val_loss: 0.5008\n"
     ]
    }
   ],
   "source": [
    "history, model = train_fashion_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec3d74",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que, en el modelo creado, **aún no se ha aplicado la función softmax**, por lo que las salidas **no representan probabilidades**. Es decir, los valores de salida pueden estar en el rango **[-∞, +∞]**.\n",
    "\n",
    "A continuación, vamos a observar los valores de salida para el **primer ejemplo** del conjunto de entrenamiento. Además, analizaremos **los valores máximos y mínimos** de todas las salidas para confirmar que **no se trata de probabilidades**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aede106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "Salidas sin softmax para el primer ejemplo:\n",
      "[-17.469152  -14.136044  -11.231507   -9.1872425 -14.566692   -6.5888705\n",
      " -14.489996    3.7524219 -10.131549    6.3474913]\n",
      "\n",
      "Valor máximo entre todas las salidas: 186.93184\n",
      "Valor mínimo entre todas las salidas: -1056.3743\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Obtener las salidas del modelo (logits) sin aplicar softmax\n",
    "train_logits = model.predict(X_train)\n",
    "\n",
    "# Mostrar los valores de salida para el primer ejemplo\n",
    "print(\"Salidas sin softmax para el primer ejemplo:\")\n",
    "print(train_logits[0])\n",
    "\n",
    "# Obtener el valor máximo y mínimo entre todas las salidas\n",
    "valor_maximo = np.max(train_logits)\n",
    "valor_minimo = np.min(train_logits)\n",
    "\n",
    "print(\"\\nValor máximo entre todas las salidas:\", valor_maximo)\n",
    "print(\"Valor mínimo entre todas las salidas:\", valor_minimo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda7ba0",
   "metadata": {},
   "source": [
    "##### ¿Qué esperar de la salida?\n",
    "\n",
    "- Un array de 10 valores para logits[0], correspondientes a las 10 clases posibles.\n",
    "- Valores fuera del rango [0, 1], confirmando que **no son probabilidades**.\n",
    "- Máximos y mínimos que podrían estar bastante alejados de 0 o 1 (por ejemplo, negativos o mayores que 1).\n",
    "\n",
    "Si queremos poder interpretar las salidas, debemos **aplicar la función Softmax para convertirlas en probabilidades**. Esta operación transforma los valores en un rango entre 0 y 1, y asegura que la suma total sea igual a 1, lo cual es característico de una distribución de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbaeb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades para el primer ejemplo:\n",
      "[4.2198710e-11 1.1826308e-09 2.1591042e-08 1.6675794e-07 7.6881296e-10\n",
      " 2.2415340e-06 8.3009993e-10 6.9456242e-02 6.4860366e-08 9.3054128e-01]\n",
      "\n",
      "Suma de probabilidades (primeros 5 ejemplos):\n",
      "[1.        1.        1.0000001 1.0000001 1.       ]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar softmax a todas las salidas\n",
    "train_pred = softmax(train_logits).numpy()\n",
    "\n",
    "# Mostrar las probabilidades del primer ejemplo\n",
    "print(\"Probabilidades para el primer ejemplo:\")\n",
    "print(train_pred[0])\n",
    "\n",
    "# Verificar que la suma de cada fila sea 1 (opcional)\n",
    "sum_train_pred = np.sum(train_pred, axis=1)\n",
    "\n",
    "# Mostrar suma de probabilidades de los primeros 5 ejemplos\n",
    "print(\"\\nSuma de probabilidades (primeros 5 ejemplos):\")\n",
    "print(sum_train_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096fec4",
   "metadata": {},
   "source": [
    "\n",
    "Como se observa, los valores de salida se encuentran en el rango **[0, 1]**. \n",
    "\n",
    "Además, si sumamos las probabilidades correspondientes a un ejemplo, el resultado debería ser **1.0**, ya que representan una distribución de probabilidad.\n",
    "\n",
    "(**Nota:** Se puede modificar el índice del ejemplo y verificar que, efectivamente, para cualquier caso, la suma de las probabilidades siempre da 1.0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf2cb0",
   "metadata": {},
   "source": [
    "#### ¿Cómo obtener las categorías estimadas?\n",
    "Para obtener que categoría que se estimó con el modelo para un ejemplo dado se debe seleccionar la categoría más probable. \n",
    "\n",
    "Para ello, es posible encontrar el índice de la salida con mayor probabilidad utilizando np.argmax(). \n",
    "\n",
    "A continuación se muestran las categorías estimadas y las reales para los 10 primeros ejemplos del dataset de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc980ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Etiquetas estimadas vs etiquetas reales (primeros 10 ejemplos):\n",
      "Ejemplo 0: Estimada = 9, Real = 9\n",
      "Ejemplo 1: Estimada = 2, Real = 2\n",
      "Ejemplo 2: Estimada = 1, Real = 1\n",
      "Ejemplo 3: Estimada = 1, Real = 1\n",
      "Ejemplo 4: Estimada = 6, Real = 6\n",
      "Ejemplo 5: Estimada = 1, Real = 1\n",
      "Ejemplo 6: Estimada = 4, Real = 4\n",
      "Ejemplo 7: Estimada = 6, Real = 6\n",
      "Ejemplo 8: Estimada = 5, Real = 5\n",
      "Ejemplo 9: Estimada = 7, Real = 7\n"
     ]
    }
   ],
   "source": [
    "test_logits = model.predict(X_test)\n",
    "\n",
    "test_pred = softmax(test_logits)\n",
    "    \n",
    "# Obtener las categorías predichas seleccionando el índice con la mayor probabilidad\n",
    "test_predicted_labels = np.argmax(test_pred, axis=1)\n",
    "\n",
    "# Mostrar las etiquetas predichas y reales para los primeros 10 ejemplos\n",
    "print(\"Etiquetas estimadas vs etiquetas reales (primeros 10 ejemplos):\")\n",
    "for i in range(10):\n",
    "    print(f\"Ejemplo {i}: Estimada = {test_predicted_labels[i]}, Real = {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082bf95",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n",
    "\n",
    "Para evaluar el desempeño del modelo de forma cuantitativa, vamos a calcular el **accuracy** tanto sobre el conjunto de entrenamiento como sobre el de testeo.\n",
    "Para ello, utilizaremos la función **accuracy_score** del módulo sklearn.metrics, que compara las etiquetas verdaderas con las predicciones generadas por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3300b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "Accuracy en entrenamiento: 0.9564\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Accuracy en testeo: 0.8878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predicciones en entrenamiento\n",
    "train_logits = model.predict(X_train)\n",
    "train_pred = np.argmax(softmax(train_logits).numpy(), axis=1)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "print(f\"Accuracy en entrenamiento: {train_acc:.4f}\")\n",
    "\n",
    "# Predicciones en testeo\n",
    "test_logits = model.predict(X_test)\n",
    "test_pred = np.argmax(softmax(test_logits).numpy(), axis=1)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(f\"Accuracy en testeo: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa6edb",
   "metadata": {},
   "source": [
    "## Observaciones finales:\n",
    "\n",
    "Si todo salió bien, en este punto el modelo de red neuronal debería alcanzar un accuracy del >90% en el conjunto de entrenamiento y un poco menor en el conjunto de testeo, lo cual indica un buen desempeño general en la tarea de clasificación multiclase. Sin embargo, habría que analizar la diferencia entre ambos valores porque puede sugerir un **ligero overfitting**, es decir, el modelo se ajustó mejor a los datos de entrenamiento que a los datos nuevos. De cualquier forma, la generalización sigue siendo aceptable.\n",
    "\n",
    "Podría evaluarse el uso de técnicas de regularización (como dropout o L2), aumento de datos o ajuste de hiperparámetros para reducir esta brecha y mejorar la robustez del modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
